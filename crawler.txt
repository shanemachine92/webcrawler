Things the crawler needs to do (methods)

SEED URL = ......

class Crawler
	def initialize ()
		@seed_url = seed_url
	end

	Given a URL, get the document

	method for getting links from the document
end

class Document
	link itself
	contents
end


class DataStructure
	arr = []
	get link from
	adds links to data structure
end


Script

New crawler instance, given the seed url
instance of data structure
looooop 
	crawler gives links from current document
	for each one, store it in data structure
	get next link from structure
	pass that one to crawler
end

Nokogiri for parsing and id-ing a-tags

# class Crawler
# 	#request the web page that you want to scrape
# 	def initialize(url)
# 		page = HTTParty.get(url)
# 		Pry.start(binding)
# 	end
# 	#Transform response into parsable Nokogiri object
# 	def create_document(page)
# 		parse_page = nokogiri::HTML(page)
# 	end

# 	def get_links_from_document(document)
# 		getting links from the document
# 	end
# end

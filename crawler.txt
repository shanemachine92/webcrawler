o) filter for domain-specific links only
o) Test relative urls
o) create a base class for url collection (what???) and make url_collection an implementation of new base class
o) Handle duplicates across domain
o) write tests

Things the crawler needs to do (methods)

SEED URL = ......

class Crawler
	def initialize ()
		@seed_url = seed_url
	end

	Given a URL, get the document

	method for getting links from the document
end

class Document
	link itself
	contents
end


class DataStructure
	arr = []
	get link from crawler
	adds links to data structure
end


Script

New crawler instance, given the seed url
instance of data structure
looooop 
	crawler gives links from current document
	for each one, store it in data structure
	get next link from structure
	pass that one to crawler
end

Nokogiri for parsing and id-ing a-tags

# class Crawler
# 	#request the web page that you want to scrape
# 	def initialize(url)
# 		page = HTTParty.get(url)
# 		Pry.start(binding)
# 	end
# 	#Transform response into parsable Nokogiri object
# 	def create_document(page)
# 		parse_page = nokogiri::HTML(page)
# 	end

# 	def get_links_from_document(document)
# 		getting links from the document
# 	end
# end

	def dump_content
		@doc.hrefs.each do |href|
			@local_copy = '#{DATA_DUMP}/#{File.basename(href)}.html'
			unless File.exists?(@local_copy)
				puts "getting #{href}"
				begin
					page_content = open(@doc.doc).read
				rescue Exception=>e
      		puts "Error: #{e}"
      		sleep 5
   			 else
         	File.open(@local_copy, 'w'){|file| file.write(@doc.doc)}
        	puts "\t...Success, saved to #{@local_copy}"
   			 ensure
      		sleep 1.0 + rand
      	end
      end
		end
	end









